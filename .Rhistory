<<<<<<< HEAD
"Where the survey was filled out [IP Address = web]",
"Percentage of the survey completed",
"The time taken to complete the survey in seconds",
"If the survey was completed",
"Date the survey was completed",
"Unique anonymous identifier for the response",
"Respondants area of work",
"Explanation of other response in work area",
"Recoded work area to group like explanations",
"Respondants description of what linked data is to a non expert",
"Quality of the descriptiong scored independently by the authors",
"If the respondant was postive, negative or neutral towards linked data, estimated by the authors",
"Respondants description of the benefits of linked data",
"The quality of the benefits answer, scored indepenently by the authors",
"If the respondants felt linked data was beneficial, not beneficial or neutral",
"Respondants can add additional comments",
"The three text responsed, linked_data_description, linked_data_benefits and additional_thoughts combined"
)
linker <- build_linker(survey, column_descriptions, column_types)
View(linker)
dict ,_ build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
dict <- build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
dict <- build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
dict <- build_dict(survey = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
dict <- build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
dict <- build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
source('~/r_projects/linked_data_survey/survey_analysis.R')
dict <- build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
edit(survey)
edit(survey)
edit(survey)
View(survey)
View(survey)
source('~/r_projects/linked_data_survey/survey_analysis.R')
dict <- build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
linker <- build_linker(survey, variable_description = column_descriptions, variable_type = column_types)
dict <- build_dict(my.data = survey, linker = linker, op)
dict <- build_dict(my.data = survey, linker = linker, option_description = NULL, prompt_varopts = FALSE)
path = "http://raw.githubusercontent.com/cdcepi/zika/master/"
path2 = "USVI/USVI_Zika/data/USVI_Zika-2017-01-03.csv"
url <- paste0(path, path2, collapse="")
my.data <- read.csv(url, header = TRUE, stringsAsFactors = FALSE)
var_desc <- c("Date when report was published", "Regional location",
"Description of regional location", "Type of case",
"A specific code for each data field", "The time period of each week",
"The type of time period", "The number of cases per data field type",
"The unit in which cases are reported")
var_type <- c(0, 1, 0, 1, 0, 0, 0, 0, 1)
linker <- build_linker(my.data, variable_description = var_desc, variable_type = var_type)
dict <- build_dict(my.data = my.data, linker = linker, option_description = NULL,
prompt_varopts = FALSE)
kable(dict, format = "html", caption = "Data dictionary for original dataset")
View(dict)
install.packages("topicmodels")
source('~/r_projects/linked_data_survey/survey_analysis.R')
library("topicmodels", lib.loc="~/R/win-library/3.3")
install.packages("topicmodels")
install.packages("slam")
library("installr", lib.loc="~/R/win-library/3.3")
updateR()
R.version()
R.Version()
is_installed <- function(package_name) is.element(package_name, installed.packages()[,1])
source('~/r_projects/linked_data_survey/survey_analysis.R')
data("AssociatedPress")
ap_lda <- LDA(AssociatedPress, k=2, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
View(ap_topics)
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
"Pride and Prejudice", "Great Expectations")
install.packages("gutenbergr")
library(gutenbergr)
books <- gutenberg_works(title %>% titles ) %>% gutenberg_download(meta_fields = "title")
books <- gutenberg_works(title %in% titles ) %>% gutenberg_download(meta_fields = "title")
install.packages("stringr")
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
by_chapter <- books %>%
group_by(title) %>%
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
View(tokenized_survey)
View(tokenized_survey)
survye$combined_words
survey$combined_words
View(tokenized_survey)
View(tokenized_survey)
frequencies <- tokenized_survey %>%
count(word, sort = TRUE) %>%
mutate(freq = n/total)
install_packages(c("topicmodels", "dplyr", "tidytext", "readr", "ggplot2", "stringr", "dataMeta"))
tokenized_survey %>% count(word, sort = TRUE)
tokenized_survey %>% count(word, sort = TRUE) %>% left_join(tokenized_survey %>% summarise(total = n()))
tokenized_survey %>% count(word, sort = TRUE) %>% group_by(response_id)
tokenized_survey %>% group_by(response_id) %>% count(word, sort = TRUE)
tokenized_survey %>% group_by(response_id) %>% count(word, sort = TRUE) %>% left_join(tokenized_survey %>% group_by(response_id) %>% summarise(total = n()))
tokenized_survey %>% group_by(response_id) %>% count(word, sort = TRUE) %>% left_join(tokenized_survey %>% group_by(response_id) %>% summarise(total = n())) %>% mutate(freq = n/total)
frequencies <- tokenized_survey %>% group_by(response_id) %>% count(word, sort = TRUE) %>% left_join(tokenized_survey %>% group_by(response_id) %>% summarise(total = n()))
frequencies
frequencies %>% cast_dtm(response_id, word, n)
words_dtm <- frequencies %>% cast_dtm(response_id, word, n)
words_dtm
survey_lda <- LDA(words_dtm, k = 4, control = list(seed = 1234))
survey_lda
survey_topics <- tidy(survey_lda, matrix = "beta")
survey_topics
top_terms <- survey_topics %>% group_by(topic)
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta)
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>^
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
frequencies <- tokenized_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(tokenized_survey %>%
group_by(response_id) %>%
summarise(total = n()))
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
source('~/r_projects/linked_data_survey/survey_analysis.R')
=======
source('~/r_projects/linked_data_survey/survey_analysis.R')
source('~/r_projects/linked_data_survey/survey_analysis.R')
get_sentiments("nrc")
tokenized_survey <- tokenized_survey %>% inner_join(get_sentiments("nrc"))
tokenized_survey$sentiment <- NULL
tokenized_survey <- tokenized_survey %>% inner_join(get_sentiments("nrc"))
tokenized_survey <- tokenized_survey %>% inner_join(get_sentiments("afinn"))
source('~/r_projects/linked_data_survey/survey_analysis.R')
get_sentiments('loughran')
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
###################
## Automate package install and load
is_installed <- function(package_name) is.element(package_name, installed.packages()[,1])
# If a package is not installed, install it. Then load the package.
install_and_load <- function(package_name) {
if(!is_installed(package_name)) {
install.packages(package_name)
}
library(package_name, character.only = TRUE)
}
install_packages <- function(packages) {
for(package in packages) {
install_and_load(package)
}
}
# Install and load libraries
<<<<<<< HEAD
install_packages(c("topicmodels", "dplyr", "tidytext", "readr", "ggplot2", "stringr"))
# Load the data
survey = read_csv('ld_results_final__for_rob_recoded_trimmed.csv', col_names = TRUE)
# Mush all th etext columns together to increase power
survey$combined_words <- paste(survey$linked_data_description, survey$linked_data_benefits, survey$additional_thoughts)
# Tokenize the words
tokenized_survey <- survey %>% unnest_tokens(word, combined_words)
# Remove stop words and join
data("stop_words")
tokenized_survey <- tokenized_survey %>% anti_join(stop_words)
# NRC changes the structre a bit since each word in the word column can get
# multiple NRC sentiments. Making a new dataframe makes it a little easier
# for the other sentiment measure.
tokenized_survey_nrc <- tokenized_survey %>% inner_join(get_sentiments("nrc"))
# The rest can just go in to the main data frame.
tokenized_survey <- tokenized_survey %>% inner_join(get_sentiments("bing"))
tokenized_survey$bing_sentiment <- tokenized_survey$sentiment
tokenized_survey$sentiment <- NULL
tokenized_survey <- tokenized_survey %>% inner_join(get_sentiments("afinn"))
tokenized_survey$afinn_sentiment <- tokenized_survey$score
tokenized_survey$score <- NULL
bing_word_counts <- tokenized_survey %>%
count(word, bing_sentiment, sort = TRUE) %>%
ungroup()
=======
install_packages(c("dplyr", "tidytext", "readr", "ggplot2", "stringr"))
survey = read_csv('ld_results_final__for_rob_recoded_trimmed.csv', col_names = TRUE)
survey$combined_words <- paste(survey$linked_data_description, survey$linked_data_benefits, survey$additional_thoughts)
tokenized_survey <- survey %>% unnest_tokens(word, combined_words)
data("stop_words")
tokenized_survey <- tokenized_survey %>% anti_join(stop_words)
tokenized_survey <- tokenized_survey %>% inner_join(get_sentiments("bing"))
tokenized_survey$bing_sentiment <- tokenized_survey$sentiment
tokenized_survey$sentiment <- NULL
tokenized_survey <- tokenized_survey %>% inner_join(get_sentiments("nrc"))
tokenized_survey$nrc_sentiment <- tokenized_survey$sentiment
source('~/r_projects/linked_data_survey/survey_analysis.R')
View(tokenized_survey_nrc)
View(tokenized_survey_nrc)
View(tokenized_survey_nrc)
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
bing_word_counts %>%
group_by(bing_sentiment) %>%
top_n(7) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = bing_sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~bing_sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
<<<<<<< HEAD
word_counts <- tokenized_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(tokenized_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta"
)
=======
View(tokenized_survey)
View(tokenized_survey)
View(tokenized_survey_nrc)
View(tokenized_survey_nrc)
View(tokenized_survey)
View(tokenized_survey)
View(survey)
View(survey)
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
library("topicmodels", lib.loc="~/Library/R/3.3/library")
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
install_url(slam_url)
R.version()
R.Version()
R.Version()
install.packages('devtools')
slam_url <- "https://cran.r-project.org/src/contrib/Archive/slam/slam_0.1-37.tar.gz"
install_url(slam_url)
slam_url <- "https://cran.r-project.org/src/contrib/Archive/slam/slam_0.1-37.tar.gz"
install_url(slam_url)
library(devtools)
install_url(slam_url)
install_url(slam_url)
install.packages("git2r")
install.packages("slam")
libary('slam')
library("slam", lib.loc="~/Library/R/3.4/library")
init_data <- data("AssociatedPress")
install.packages("topicmodels")
library("topicmodels", lib.loc="~/Library/R/3.4/library")
init_data <- data("AssociatedPress")
init_data
ap_lda <- LDA(init_data, k = 2, control = list(seed = 1234))
data("AssociatedPress")
<<DocumentTermMatrix
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
install_packages(c("devtools", "dplyr", "tidytext", "readr", "ggplot2", "stringr", "topicmodels"))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
word_counts <- tokenized_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(tokenized_survey %>%
group_by(response_id) %>%
summarise(total = n()))
<<<<<<< HEAD
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
data("stop_words")
stop_words <- data("stop_words")
stop_words
stop_words
View(stop_words)
data("stop_words")
source('~/r_projects/linked_data_survey/survey_analysis.R')
help("count")
tokenized_survey %>% count(word, sort = TRUE)
temp <- tokenized_survey %>% count(word, sort = TRUE)
View(temp)
###################
=======
##################
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
## Automate package install and load
is_installed <- function(package_name) is.element(package_name, installed.packages()[,1])
# If a package is not installed, install it. Then load the package.
install_and_load <- function(package_name) {
if(!is_installed(package_name)) {
install.packages(package_name)
}
library(package_name, character.only = TRUE)
}
install_packages <- function(packages) {
for(package in packages) {
install_and_load(package)
}
}
# Install and load libraries
<<<<<<< HEAD
install_packages(c("topicmodels", "dplyr", "tidytext", "readr", "ggplot2", "stringr"))
=======
install_packages(c("topicmodels", "dplyr", "tidytext", "readr", "ggplot2", "stringr", "dataMeta"))
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
# Load the data
survey = read_csv('ld_results_final__for_rob_recoded_trimmed.csv', col_names = TRUE)
# Mush all th etext columns together to increase power
survey$combined_words <- paste(survey$linked_data_description, survey$linked_data_benefits, survey$additional_thoughts)
# Tokenize the words
tokenized_survey <- survey %>% unnest_tokens(word, combined_words)
# Remove stop words and join
data("stop_words")
tokenized_survey <- tokenized_survey %>% anti_join(stop_words)
<<<<<<< HEAD
temp <- tokenized_survey %>% count(word, sort = TRUE)
View(temp)
=======
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
word_counts <- tokenized_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(tokenized_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
custom_stops <- c(
'data', 'library', 'information'
)
topic_survey <- tokenized_survey %>% anti_join(custom_stops)
word_counts <- topic_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(topic_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
=======
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 2, control = list(seed = 1234))
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
topic_survey <- tokenized_survey %>% anti_join(custom_stops)
View(stop_words)
custom_stops <- cbind(c(
'data', 'library', 'information'
))
View(custom_stops)
custom_stops <- data.frame(word)
View(custom_stops)
word <- c('librar', 'information', 'link', 'data')
custom_stops <- data.frame(word)
View(custom_stops)
custom_stops$lexicon <- 'custom'
word <- c('librar', 'information', 'link', 'data', 'linked')
custom_stops <- data.frame(word)
custom_stops$lexicon <- 'custom'
topic_survey <- tokenized_survey %>% anti_join(custom_stops)
word_counts <- topic_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(topic_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
=======
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
stop_words
tail(stop_words)
c(stop_words, ["data", "custom"])
c(stop_words, c("data", "custom"))
stop_words
add_row(stop_words, word = "library", lexicon = "custom", .before = 0)
data("stop_words")
stop_words
new_stops = c("data", "linked", "information", "library")
add_row(stop_words, word = new_stops, lexicon = "custom", .before = 0)
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
word <- c('library', 'libraries', 'information', 'link', 'data', 'linked')
custom_stops <- data.frame(word)
custom_stops$lexicon <- 'custom'
topic_survey <- tokenized_survey %>% anti_join(custom_stops)
word_counts <- topic_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(topic_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
=======
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
###
#
# Topic modeling
#
###
word <- c('library', 'libraries', 'information', 'link', 'data', 'linked', 'na', '2')
custom_stops <- data.frame(word)
custom_stops$lexicon <- 'custom'
topic_survey <- tokenized_survey %>% anti_join(custom_stops)
word_counts <- topic_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(topic_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#
# Topic modeling
#
###
word <- c('library', 'libraries', 'information', 'link', 'data', 'linked', 'na', '2')
custom_stops <- data.frame(word)
custom_stops$lexicon <- 'custom'
topic_survey <- tokenized_survey %>% anti_join(custom_stops)
word_counts <- topic_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(topic_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
=======
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
tokenized_survey_nrc <- tokenized_survey %>% inner_join(get_sentiments("nrc"))
View(tokenized_survey_nrc)
View(tokenized_survey_nrc)
tokenized_survey_nrc <- tokenized_survey %>% inner_join(get_sentiments("nrc"))
View(tokenized_survey_nrc)
tokenized_survey_bing <- tokenized_survey %>% inner_join(get_sentiments("bing"))
View(tokenized_survey_bing)
tokenized_survey_afinn <- tokenized_survey %>% inner_join(get_sentiments("afinn"))
View(tokenized_survey_afinn)
bing_word_counts <-  tokenized_survey_bing %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(tokenized_survey_bing %>%
group_by(response_id) %>%
summarise(total = n()))
bing_word_counts_dtm <- bing_word_counts %>% cast_dtm(response_id, word, n)
bing_responses_lda <- LDA(bing_word_counts_dtm, k = 4, control = list(seed = 1234))
bing_survey_topics <- tidy(bing_responses_lda, matrix = "beta")
bing_top_terms <- bing_survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
bing_top_terms %>%
=======
View(stop_words)
View(stop_words)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
bing_word_counts <-  tokenized_survey_bing %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(tokenized_survey_bing %>%
group_by(response_id) %>%
summarise(total = n()))
bing_word_counts_dtm <- bing_word_counts %>% cast_dtm(response_id, word, n)
bing_responses_lda <- LDA(bing_word_counts_dtm, k = 4, control = list(seed = 1234))
bing_survey_topics <- tidy(bing_responses_lda, matrix = "beta")
bing_top_terms <- bing_survey_topics %>% group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
bing_top_terms %>%
=======
data("stop_words")
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
top_terms %>%
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
View(tokenized_survey_bing)
View(tokenized_survey_bing)
View(tokenized_survey_nrc)
View(tokenized_survey_nrc)
nrc_word_counts <-  tokenized_survey_nrc %>%
group_by(response_id) %>%
count(sentiment, sort = TRUE) %>%
left_join(tokenized_survey_nrc %>%
group_by(response_id) %>%
summarise(total = n()))
View(nrc_word_counts)
nrc_word_counts_dtm <- nrc_word_counts %>% cast_dtm(response_id, sentiment, n)
nrc_responses_lda <- LDA(nrc_word_counts_dtm, k = 4, control = list(seed = 1234))
nrc_survey_topics <- tidy(nrc_responses_lda, matrix = "beta")
nrc_top_terms <- nrc_survey_topics %>% group_by(topic) %>%
top_n(5, beta) %>%
ungroup() %>%
arrange(topic, -beta)
nrc_top_terms %>%
=======
View(stop_words)
View(stop_words)
source('~/Documents/r_projects/linked_data_survey/survey_analysis.R')
word_counts <- tokenized_survey %>%
group_by(response_id) %>%
count(word, sort = TRUE) %>%
left_join(tokenized_survey %>%
group_by(response_id) %>%
summarise(total = n()))
word_counts_dtm <- word_counts %>% cast_dtm(response_id, word, n)
responses_lda <- LDA(word_counts_dtm, k = 4, control = list(seed = 1234))
survey_topics <- tidy(responses_lda, matrix = "beta")
top_terms <- survey_topics %>% group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
<<<<<<< HEAD
=======
tokenized_survey <- survey %>% unnest_tokens(word, combined_words)
View(tokenized_survey)
View(tokenized_survey)
>>>>>>> a0c528f5d53f4b9394393fbdfd4e005935f80bef
